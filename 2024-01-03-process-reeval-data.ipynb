{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from pathlib import Path\n",
    "from scipy.stats import mannwhitneyu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rpy2\n",
    "# Configure for notebook use.\n",
    "import rpy2.ipython.html\n",
    "rpy2.ipython.html.init_printing()\n",
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "library(ggplot2)\n",
    "library(dplyr)\n",
    "library(patchwork)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c0a = \"accuracy (validation)\"\n",
    "c0b = \"accuracy (test)\"\n",
    "c1 = \"multiply-adds\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General 'knowledge'\n",
    "improvement_direction = {\n",
    "    \"accuracy\": 1,\n",
    "    \"accuracy (validation)\": 1,\n",
    "    \"accuracy (test)\": 1,\n",
    "    \"loss\": -1,\n",
    "    \"loss (validation)\": -1,\n",
    "    \"loss (test)\": -1,\n",
    "    \"loss-clip\": -1,\n",
    "    \"total bytes\": -1,\n",
    "    \"total_memory_bytes\": -1, # dict name\n",
    "    \"multiply-adds\": -1,\n",
    "    \"total_mult_adds\": -1, # dict name\n",
    "    # \"genotype\": 0, # -- not a criterion\n",
    "}\n",
    "best_possible_value_imagenet = {\n",
    "    \"accuracy\": 1.0,\n",
    "    \"loss\": 0.0,\n",
    "    \"accuracy (validation)\": 1.0,\n",
    "    \"loss (validation)\": 0.0,\n",
    "    \"accuracy (test)\": 1.0,\n",
    "    \"loss (test)\": 0.0,\n",
    "    \"loss-clip\": 0.0,\n",
    "    \"total bytes\": 0.0,\n",
    "    \"total_memory_bytes\": 0.0,\n",
    "    \"multiply-adds\": 0.0,\n",
    "    \"total_mult_adds\": 0.0, \n",
    "}\n",
    "\n",
    "best_possible_value_voc = {\n",
    "    \"accuracy\": 1.0,\n",
    "    \"loss\": 0.0,\n",
    "    \"accuracy (validation)\": 1.0,\n",
    "    \"loss (validation)\": 0.0,\n",
    "    \"accuracy (test)\": 1.0,\n",
    "    \"loss (test)\": 0.0,\n",
    "    \"loss-clip\": 0.0,\n",
    "    \"total bytes\": 0.0,\n",
    "    \"total_memory_bytes\": 0.0,\n",
    "    \"multiply-adds\": 0.0,\n",
    "    \"total_mult_adds\": 0.0, \n",
    "}\n",
    "\n",
    "task_relabeling = {\n",
    "    \"voc\": \"VOC\",\n",
    "    \"imagenet (a)\": \"ImageNet (a)\",\n",
    "    \"imagenet (b)\": \"ImageNet (b)\",\n",
    "}\n",
    "\n",
    "# # Task specific settings\n",
    "# if task == \"imagenet (a)\":\n",
    "#     # imagenet-a\n",
    "#     folder = Path(\"./2024-01-02-results/imagenet_a/\")\n",
    "#     assert folder.exists()\n",
    "#     run_folder = folder / \"exp-imagenet-a\"\n",
    "#     assert run_folder.exists()\n",
    "#     files = list(run_folder.glob(\"*.arrow\"))\n",
    "#     reference_file = folder / \"stitched-imagenet-a-reference.arrow\"\n",
    "#     # some task-specific tidbits\n",
    "#     min_accuracy = 0.7\n",
    "#     best_possible_value[\"accuracy (validation)\"] = 0.80\n",
    "#     best_possible_value[\"accuracy (test)\"] = 0.85\n",
    "# elif task == \"segmentation\":\n",
    "#     folder = Path(\"./2024-01-02-results/segmentation/\")\n",
    "#     assert folder.exists()\n",
    "#     run_folder = folder / \"exp-voc\"\n",
    "#     assert run_folder.exists()\n",
    "#     files = list(run_folder.glob(\"*.arrow\"))\n",
    "#     reference_file = folder / \"stitched-voc-reference.arrow\"\n",
    "#     # some task-specific tidbits\n",
    "#     min_accuracy = 0.90\n",
    "# else:\n",
    "#     raise ValueError(\"Unknown task\")\n",
    "# if c0 == \"accuracy\":\n",
    "#     min_performance = min_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapoints = (pl.concat([\n",
    "    pl.read_ndjson(\"./2024-01-02-results/imagenet_a/imagenet_a-front.reeval.jsonl\").lazy().with_columns(pl.lit(\"imagenet (a)\").alias(\"task\")),\n",
    "    pl.read_ndjson(\"./2024-01-02-results/segmentation/segmentation-front.reeval.jsonl\").lazy().with_columns(pl.lit(\"voc\").alias(\"task\")),\n",
    "    pl.read_ndjson(\"./2024-01-04/imagenet_b-front.reeval.jsonl\").lazy().with_columns(pl.lit(\"imagenet (b)\").alias(\"task\")),\n",
    "], how=\"diagonal\")\n",
    "    .with_columns([\n",
    "        pl.col(\"set\").str.replace(\"_\", \"-\"),\n",
    "        pl.col(\"accuracy\").alias(\"accuracy (validation)\"),\n",
    "        pl.col(\"reeval-result\").struct.field(\"accuracy\").alias(\"accuracy (test)\"),\n",
    "        ])\n",
    "    .drop(\"reeval-result\")\n",
    "    .with_columns([\n",
    "        pl.col(\"set\").replace({\"SGA\": \"GA\"}),\n",
    "        pl.col(\"task\").replace(task_relabeling),\n",
    "        ])\n",
    ").collect()\n",
    "datapoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Random Search had a bug that caused it to reevaluate the same solution over and over again from a certain point onwards.\n",
    "> **Exclude** these results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapoints = datapoints.filter(pl.col(\"set\") != \"RS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare for front computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tools for computing fronts\n",
    "def maybe_over(a, o):\n",
    "    if len(o) == 0: return a\n",
    "    else: return a.over(o)\n",
    "\n",
    "def compute_pareto(df, group_vars, c0, c1, dmul=1):\n",
    "    return (df.sort([c0, c1], descending=[improvement_direction[c0] * dmul > 0, improvement_direction[c1] * dmul > 0])\n",
    "        .with_columns((pl.col(c1) * -improvement_direction[c1] * dmul).alias(\"c1-min\"))\n",
    "        .with_columns(maybe_over((pl.col(\"c1-min\")).cum_min(), group_vars).alias(\"mv\"))\n",
    "        .with_columns((maybe_over(pl.col(\"c1-min\") < pl.col(\"mv\").shift(1), group_vars).alias(\"is pareto\")).fill_null(True))\n",
    "        .filter(pl.col(\"is pareto\"))\n",
    "    )\n",
    "\n",
    "def compute_2d_hv(df_pareto, ref, axis_scale, group_vars, c0, c1):\n",
    "    # note - df_pareto is a df created using compute_pareto\n",
    "    dhva = (df_pareto.sort(c0, descending=improvement_direction[c0] < 0)\n",
    "        # Samples worse than reference point do not contribute.\n",
    "        # .filter(improvement_direction[c0] * pl.col(c0) > improvement_direction[c0] * ref[0])\n",
    "        # .filter(improvement_direction[c1] * pl.col(c1) > improvement_direction[c1] * ref[1])\n",
    "        .with_columns(\n",
    "        [\n",
    "            maybe_over( improvement_direction[c0] * (pl.col(c0) - pl.col(c0).shift(1).fill_null(ref[0])) / axis_scale[0], group_vars).alias(\"slice_width\"),\n",
    "            maybe_over( improvement_direction[c1] * (pl.col(c1) - ref[1]) / axis_scale[1], group_vars).alias(\"slice_height\"),\n",
    "        ])\n",
    "        .select([pl.col(group_vars), (pl.col(\"slice_width\") * pl.col(\"slice_height\")).alias(\"hv_contrib\")])\n",
    "        .group_by(group_vars).agg(pl.col(\"hv_contrib\").sum()))\n",
    "    return dhva\n",
    "\n",
    "def get_transformed_front(per_run_front, grouping, c0, c1, include_poly=False):\n",
    "    # Create a dataframe for plotting in R\n",
    "    per_run_front_b = (per_run_front.lazy()\n",
    "        # Add a tag so that we can track which samples were original - and which ones were added for sake\n",
    "        # of continuing the lines.\n",
    "        .with_columns(pl.lit(1.0).alias(\"is_original\"))\n",
    "        # For each run include an additional two rows:\n",
    "        # Repeat best per objective, but replace the other objective with -Inf - as to plot towards the axes.\n",
    "        .merge_sorted(per_run_front.lazy()\n",
    "                    .with_columns([(pl.col(c0) * improvement_direction[c0]).alias(\"c0-n\"),\n",
    "                                    pl.lit(-np.Inf * improvement_direction[c1]).alias(c1),\n",
    "                                    pl.lit(0.0).alias(\"is_original\")])\n",
    "                    .group_by(grouping, maintain_order=True)\n",
    "                    .agg(pl.all().sort_by(\"c0-n\").last())\n",
    "                    .select(per_run_front.columns + [\"is_original\"]), \"file\")\n",
    "        .merge_sorted(per_run_front.lazy()\n",
    "                    .with_columns([(pl.col(c1) * improvement_direction[c1]).alias(\"c1-n\"),\n",
    "                                    pl.lit(-np.Inf * improvement_direction[c0]).alias(c0),\n",
    "                                    pl.lit(0.0).alias(\"is_original\")])\n",
    "                    .group_by(grouping, maintain_order=True)\n",
    "                    .agg(pl.all().sort_by(\"c1-n\").last())\n",
    "                    .select(per_run_front.columns + [\"is_original\"]), \"file\")\n",
    "        # Add c0 and c1 as a named column\n",
    "        .with_columns([pl.col(c0).alias(\"c0\"), pl.col(c1).alias(\"c1\")])\n",
    "        # Sort, for good measure\n",
    "        .sort(grouping + [c0, c1]).collect())\n",
    "    if include_poly:\n",
    "        # Also derive a polygon-boundary\n",
    "        poly_b = pl.concat([\n",
    "            per_run_front_b.lazy().select(\n",
    "                pl.col(grouping),\n",
    "                pl.col(\"c0\"),\n",
    "                pl.col(\"c1\"),\n",
    "            ).with_row_count().with_columns(pl.col(\"row_nr\") * 2 + 1),\n",
    "            per_run_front_b.lazy().select(\n",
    "                pl.col(grouping),\n",
    "                pl.col(\"c0\").shift(1).fill_null(-np.Inf * improvement_direction[c0]).over(grouping),\n",
    "                pl.col(\"c1\"),\n",
    "            ).with_row_count().with_columns(pl.col(\"row_nr\") * 2),\n",
    "            per_run_front_b.lazy().group_by(grouping).agg(\n",
    "                pl.lit(-np.Inf * improvement_direction[c0]).alias(\"c0\"),\n",
    "                pl.lit(-np.Inf * improvement_direction[c1]).alias(\"c1\"),\n",
    "                pl.lit(-1).alias(\"row_nr\"),\n",
    "            )\n",
    "        ], how=\"diagonal_relaxed\").sort(\"row_nr\").collect()\n",
    "        return per_run_front_b.to_pandas(), poly_b.to_pandas()\n",
    "\n",
    "    return per_run_front_b.to_pandas()\n",
    "\n",
    "def compute_reference_domination_poly(reference_points, grouping, c0, c1):\n",
    "    return (pl.concat([\n",
    "            reference_points.lazy().select(\n",
    "                pl.col(grouping),\n",
    "                pl.col(\"c0\"),\n",
    "                pl.col(\"c1\"),\n",
    "            ).with_row_count().with_columns(pl.col(\"row_nr\") * 2),\n",
    "            reference_points.lazy().select(\n",
    "                pl.col(grouping),\n",
    "                pl.col(\"c0\").shift(-1).fill_null(np.Inf * improvement_direction[c0]).over(grouping),\n",
    "                pl.col(\"c1\"),\n",
    "            ).with_row_count().with_columns(pl.col(\"row_nr\") * 2 + 1),\n",
    "            reference_points.lazy().group_by(grouping).agg(\n",
    "                (pl.col(\"c0\") * improvement_direction[c0]).min(),\n",
    "                pl.lit(np.Inf * improvement_direction[c1]).alias(\"c1\"),\n",
    "                pl.lit(-1).alias(\"row_nr\"),\n",
    "            ),\n",
    "            reference_points.lazy().group_by(grouping).agg(\n",
    "                pl.lit(np.Inf * improvement_direction[c0]).alias(\"c0\"),\n",
    "                pl.lit(np.Inf * improvement_direction[c1]).alias(\"c1\"),\n",
    "                pl.lit(-2).alias(\"row_nr\"),\n",
    "            ),\n",
    "        ], how=\"diagonal_relaxed\").sort(grouping + [\"row_nr\"])).collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute front over all evaluated solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original\n",
    "all_data = datapoints\n",
    "total_front_a = compute_pareto(all_data.lazy().sort([\"task\", c0a, c1]), [\"task\"], c0a, c1).drop(\"genotype\").collect()\n",
    "reference_a = all_data.filter(pl.col(\"file\") == \"reference\").drop(\"genotype\").with_columns(\n",
    "    pl.col(c0a).alias(\"c0\"), pl.col(c1).alias(\"c1\")\n",
    ")\n",
    "total_front_b = compute_pareto(all_data.lazy().sort([\"task\", c0b, c1]), [\"task\"], c0b, c1).collect()\n",
    "reference_b = all_data.filter(pl.col(\"file\") == \"reference\").drop(\"genotype\").with_columns(\n",
    "    pl.col(c0b).alias(\"c0\"), pl.col(c1).alias(\"c1\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_per_run_front = get_transformed_front(total_front_a, [\"task\"], c0a, c1)\n",
    "reference = reference_a.to_pandas()\n",
    "reference_poly = compute_reference_domination_poly(compute_pareto(reference_a, [\"task\"], c0a, c1, dmul=-1), [\"task\"], c0a, c1).to_pandas()\n",
    "c0 = c0a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_front_a\n",
    "pd_per_run_front"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i pd_per_run_front -i reference_poly -i reference -i c0 -i c1 -w 500 -h 300\n",
    "\n",
    "# Remove additional samples added to continue the lines to the axis edge\n",
    "pd_per_run_front_excl_edges <- pd_per_run_front |> filter(`is_original` > 0.5)\n",
    "\n",
    "pla <- ggplot(pd_per_run_front, aes(x = `c0`, y=`c1`)) +\n",
    "    geom_polygon(data = reference_poly, fill=\"orange\", alpha=0.2) +\n",
    "    geom_step(alpha=0.3, direction = \"vh\") +\n",
    "    geom_point(data = pd_per_run_front_excl_edges, alpha=0.3) +\n",
    "    geom_point(data = reference, color=\"orange\", group=\"reference\", shape=4, size=2, stroke=2) +\n",
    "    labs(x = c0, y = c1, color = \"approach\") +\n",
    "    facet_wrap(`task` ~ ., scales=\"free\") +\n",
    "    theme_bw() +\n",
    "    theme(\n",
    "      legend.position=\"bottom\",\n",
    "      axis.text.x = element_text(angle = 45, vjust = 1, hjust=1),\n",
    "      plot.background = element_rect(fill='transparent', color=NA),\n",
    "      strip.background = element_blank())\n",
    "pla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_per_run_front, poly = get_transformed_front(total_front_b.sort([\"task\", c0b, c1]), [\"task\"], c0b, c1, include_poly=True)\n",
    "reference_poly = compute_reference_domination_poly(compute_pareto(reference_b, [\"task\"], c0b, c1, dmul=-1), [\"task\"], c0b, c1).to_pandas()\n",
    "reference = reference_b.to_pandas()\n",
    "c0 = c0b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i pd_per_run_front -i reference -i reference_poly -i c0 -i c1 -w 500 -h 300\n",
    "\n",
    "# Remove additional samples added to continue the lines to the axis edge\n",
    "pd_per_run_front_excl_edges <- pd_per_run_front |> filter(`is_original` > 0.5)\n",
    "\n",
    "plb <- ggplot(pd_per_run_front, aes(x = `c0`, y=`c1`)) +\n",
    "    geom_polygon(data = reference_poly, fill=\"orange\", alpha=0.2) +\n",
    "    geom_step(alpha=0.3, direction = \"vh\") +\n",
    "    geom_point(data = pd_per_run_front_excl_edges, alpha=0.3) +\n",
    "    geom_point(data = reference, color=\"orange\", group=\"reference\", shape=4, size=2, stroke=2) +\n",
    "    labs(x = c0, y = c1, color = \"approach\") +\n",
    "    facet_wrap(`task` ~ ., scales=\"free\") +\n",
    "    theme_bw() +\n",
    "    theme(\n",
    "      legend.position=\"bottom\",\n",
    "      axis.text.x = element_text(angle = 45, vjust = 1, hjust=1),\n",
    "      plot.background = element_rect(fill='transparent', color=NA),\n",
    "      strip.background = element_blank())\n",
    "plb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -w 1000 -h 300\n",
    "pla | plb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute front per run\n",
    "With respect to a reference depending on all evaluated solutions / predetermined bounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compute reference point & scale for hypervolume\n",
    "# # For the reference point we use the nadir: the worst combination of objectives.\n",
    "# # Note, that this is computed /after/ a minimal fitness filter.\n",
    "# worst_perf_c0, worst_perf_c1 = all_data.select(\n",
    "#     (improvement_direction[c0] * (improvement_direction[c0] * pl.col(c0)).min()).alias(c0),\n",
    "#     (improvement_direction[c1] * (improvement_direction[c1] * pl.col(c1)).min()).alias(c1)\n",
    "#     ).collect().row(0)\n",
    "# # Override, if we have a filter threshold.\n",
    "# if min_performance is not None:\n",
    "#     worst_perf_c0 = min_performance\n",
    "# scale_c0 = -improvement_direction[c0] * (worst_perf_c0 - best_possible_value[c0])\n",
    "# scale_c1 = -improvement_direction[c1] * (worst_perf_c1 - best_possible_value[c1])\n",
    "\n",
    "# hv_reference_point = (worst_perf_c0, worst_perf_c1)\n",
    "# hv_scale = (scale_c0, scale_c1)\n",
    "\n",
    "# print(f\"Reference point: ({worst_perf_c0}, {worst_perf_c1}) - Scale: ({scale_c0}, {scale_c1})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = datapoints\n",
    "fronts_a = compute_pareto(all_data.lazy().sort([\"task\", \"file\", c0a, c1]), [\"task\", \"file\"], c0a, c1).drop(\"genotype\").collect()\n",
    "pd_fronts = get_transformed_front(fronts_a, [\"task\", \"file\"], c0a, c1)\n",
    "reference = all_data.filter(pl.col(\"file\") == \"reference\").drop(\"genotype\").with_columns(\n",
    "    pl.col(c0a).alias(\"c0\"), pl.col(c1).alias(\"c1\")\n",
    ")\n",
    "reference_poly = compute_reference_domination_poly(compute_pareto(reference, [\"task\"], c0a, c1, dmul=-1), [\"task\"], c0a, c1).to_pandas()\n",
    "reference = reference.to_pandas()\n",
    "c0 = c0a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_multiplier = pl.from_pandas(pd_fronts).lazy().select(pl.col(\"c1\").min().log10().floor()).collect()\n",
    "scale_multiplier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i pd_fronts -i reference -i reference_poly -i c0 -i c1 -w 500 -h 300\n",
    "pal <- palette(\"r4\")[2:6]\n",
    "scientific_10 <- function(x) {   parse(text=gsub(\"e\\\\+*\", \" %*% 10^\", scales::scientific_format()(x))) }\n",
    "# scientific_10 <- function(x) { x / 10^9 } \n",
    "\n",
    "# Remove additional samples added to continue the lines to the axis edge\n",
    "pd_fronts_no_ref <- pd_fronts |> filter(`set` != \"reference\")\n",
    "pd_fronts_excl_edges <- pd_fronts_no_ref |> filter(`is_original` > 0.5)\n",
    "\n",
    "pla <- ggplot(pd_fronts_no_ref, aes(x = `c0`, y=`c1`, color=`set`, group=`file`)) +\n",
    "    geom_polygon(data = reference_poly, aes(x=`c0`, y=`c1`), fill=\"orange\", alpha=0.2, inherit.aes=FALSE) +\n",
    "    geom_step(alpha=0.3, direction = \"vh\") +\n",
    "    geom_point(data = pd_fronts_excl_edges, alpha=0.3) +\n",
    "    geom_point(data = reference, color=\"orange\", shape=4, size=2, stroke=2) +\n",
    "    scale_color_manual(values=pal) +\n",
    "    scale_y_continuous(label=scientific_10) +\n",
    "    labs(x = c0, y = c1, color = \"approach: \") +\n",
    "    facet_wrap(`task` ~ ., scales=\"free\") +\n",
    "    theme_bw() +\n",
    "    theme(\n",
    "      legend.position=\"bottom\",\n",
    "      axis.text.x = element_text(angle = 45, vjust = 1, hjust=1),\n",
    "      plot.background = element_rect(fill='transparent', color=NA),\n",
    "      strip.background = element_blank())\n",
    "pla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = datapoints\n",
    "fronts_a = compute_pareto(all_data.lazy(), [\"task\", \"file\"], c0b, c1).drop(\"genotype\").collect()\n",
    "pd_fronts = get_transformed_front(fronts_a, [\"task\", \"file\"], c0b, c1)\n",
    "reference = all_data.filter(pl.col(\"file\") == \"reference\").drop(\"genotype\").with_columns(\n",
    "    pl.col(c0b).alias(\"c0\"), pl.col(c1).alias(\"c1\")\n",
    ")\n",
    "reference_poly = compute_reference_domination_poly(compute_pareto(reference, [\"task\"], c0b, c1, dmul=-1), [\"task\"], c0b, c1).to_pandas()\n",
    "reference = reference.to_pandas()\n",
    "c0 = c0b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i pd_fronts -i reference -i reference_poly -i c0 -i c1 -w 500 -h 300\n",
    "\n",
    "# Remove additional samples added to continue the lines to the axis edge\n",
    "pd_fronts_no_ref <- pd_fronts |> filter(`set` != \"reference\")\n",
    "pd_fronts_excl_edges <- pd_fronts_no_ref |> filter(`is_original` > 0.5)\n",
    "\n",
    "plb <- ggplot(pd_fronts_no_ref, aes(x = `c0`, y=`c1`, color=`set`, group=`file`)) +\n",
    "    geom_polygon(data = reference_poly, aes(x=`c0`, y=`c1`), fill=\"orange\", alpha=0.2, inherit.aes=FALSE) +\n",
    "    geom_step(alpha=0.3, direction = \"vh\") +\n",
    "    geom_point(data = pd_fronts_excl_edges, alpha=0.3) +\n",
    "    geom_point(data = reference, color=\"orange\", shape=4, size=2, stroke=2) +\n",
    "    scale_color_manual(values=pal) +\n",
    "    scale_y_continuous(label=scientific_10) +\n",
    "    labs(x = c0, y = c1, color = \"approach: \") +\n",
    "    facet_wrap(`task` ~ ., scales=\"free\") +\n",
    "    theme_bw() +\n",
    "    theme(\n",
    "      legend.position=\"bottom\",\n",
    "      axis.text.x = element_text(angle = 45, vjust = 1, hjust=1),\n",
    "      plot.background = element_rect(fill='transparent', color=NA),\n",
    "      strip.background = element_blank())\n",
    "plb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -w 1000 -h 260\n",
    "pl <- (pla | plb) + plot_layout(guides = \"collect\") & theme(legend.position=\"bottom\")\n",
    "# ggsave(\"fronts-in-ab-seg-dominates-parent-marker.pdf\", device = cairo_pdf)\n",
    "# ggsave(\"fronts-in-a-seg-dominates-parent-marker.pdf\", device = cairo_pdf)\n",
    "pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b solo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = datapoints\n",
    "fronts_a = compute_pareto(all_data.lazy().sort([\"file\", c0a, c1]), [\"file\"], c0a, c1).drop(\"genotype\").collect()\n",
    "pd_fronts = get_transformed_front(fronts_a, [\"file\"], c0a, c1)\n",
    "reference = all_data.filter(pl.col(\"file\") == \"reference\").drop(\"genotype\").with_columns(\n",
    "    pl.col(c0a).alias(\"c0\"), pl.col(c1).alias(\"c1\")\n",
    ")\n",
    "reference_poly = compute_reference_domination_poly(compute_pareto(reference, [\"task\"], c0a, c1, dmul=-1), [\"task\"], c0a, c1).to_pandas()\n",
    "reference = reference.to_pandas()\n",
    "c0 = c0a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_multiplier = pl.from_pandas(pd_fronts).lazy().select(pl.col(\"c1\").min().log10().floor()).collect()\n",
    "scale_multiplier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i pd_fronts -i reference -i reference_poly -i c0 -i c1 -w 500 -h 300\n",
    "pal <- palette(\"r4\")[c(2,4)]\n",
    "scientific_10 <- function(x) {   parse(text=gsub(\"e\\\\+*\", \" %*% 10^\", scales::scientific_format()(x))) }\n",
    "# scientific_10 <- function(x) { x / 10^9 } \n",
    "\n",
    "# Remove additional samples added to continue the lines to the axis edge\n",
    "pd_fronts_no_ref <- pd_fronts |> filter(`set` != \"reference\")\n",
    "pd_fronts_excl_edges <- pd_fronts_no_ref |> filter(`is_original` > 0.5)\n",
    "\n",
    "pla <- ggplot(pd_fronts_no_ref, aes(x = `c0`, y=`c1`, color=`set`, group=`file`)) +\n",
    "    geom_polygon(data = reference_poly, aes(x=`c0`, y=`c1`), fill=\"orange\", alpha=0.2, inherit.aes=FALSE) +\n",
    "    geom_step(alpha=0.3, direction = \"vh\") +\n",
    "    geom_point(data = pd_fronts_excl_edges, alpha=0.3) +\n",
    "    geom_point(data = reference, color=\"orange\", shape=4, size=2, stroke=2) +\n",
    "    scale_color_manual(values=pal) +\n",
    "    scale_y_continuous(label=scientific_10) +\n",
    "    labs(x = c0, y = c1, color = \"approach: \") +\n",
    "    facet_wrap(`task` ~ ., scales=\"free\") +\n",
    "    theme_bw() +\n",
    "    theme(\n",
    "      legend.position=\"bottom\",\n",
    "      axis.text.x = element_text(angle = 45, vjust = 1, hjust=1),\n",
    "      plot.background = element_rect(fill='transparent', color=NA),\n",
    "      strip.background = element_blank())\n",
    "pla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = datapoints\n",
    "fronts_a = compute_pareto(all_data.lazy(), [\"file\"], c0b, c1).drop(\"genotype\").collect()\n",
    "pd_fronts = get_transformed_front(fronts_a, [\"file\"], c0b, c1)\n",
    "reference = all_data.filter(pl.col(\"file\") == \"reference\").drop(\"genotype\").with_columns(\n",
    "    pl.col(c0b).alias(\"c0\"), pl.col(c1).alias(\"c1\")\n",
    ")\n",
    "reference_poly = compute_reference_domination_poly(compute_pareto(reference, [\"task\"], c0b, c1, dmul=-1), [\"task\"], c0b, c1).to_pandas()\n",
    "reference = reference.to_pandas()\n",
    "c0 = c0b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i pd_fronts -i reference -i reference_poly -i c0 -i c1 -w 500 -h 300\n",
    "\n",
    "# Remove additional samples added to continue the lines to the axis edge\n",
    "pd_fronts_no_ref <- pd_fronts |> filter(`set` != \"reference\")\n",
    "pd_fronts_excl_edges <- pd_fronts_no_ref |> filter(`is_original` > 0.5)\n",
    "\n",
    "plb <- ggplot(pd_fronts_no_ref, aes(x = `c0`, y=`c1`, color=`set`, group=`file`)) +\n",
    "    geom_polygon(data = reference_poly, aes(x=`c0`, y=`c1`), fill=\"orange\", alpha=0.2, inherit.aes=FALSE) +\n",
    "    geom_step(alpha=0.3, direction = \"vh\") +\n",
    "    geom_point(data = pd_fronts_excl_edges, alpha=0.3) +\n",
    "    geom_point(data = reference, color=\"orange\", shape=4, size=2, stroke=2) +\n",
    "    scale_color_manual(values=pal) +\n",
    "    scale_y_continuous(label=scientific_10) +\n",
    "    labs(x = c0, y = c1, color = \"approach: \") +\n",
    "    facet_wrap(`task` ~ ., scales=\"free\") +\n",
    "    theme_bw() +\n",
    "    theme(\n",
    "      legend.position=\"bottom\",\n",
    "      axis.text.x = element_text(angle = 45, vjust = 1, hjust=1),\n",
    "      plot.background = element_rect(fill='transparent', color=NA),\n",
    "      strip.background = element_blank())\n",
    "plb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -w 500 -h 260\n",
    "pl <- (pla | plb) + plot_layout(guides = \"collect\") & theme(legend.position=\"bottom\")\n",
    "# ggsave(\"fronts-in-ab-seg-dominates-parent-marker.pdf\")\n",
    "ggsave(\"fronts-in-b-dominates-parent-marker.pdf\", device = cairo_pdf)\n",
    "pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rescaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = datapoints\n",
    "fronts_a = compute_pareto(all_data.lazy().sort([\"task\", \"file\", c0a, c1]), [\"task\", \"file\"], c0a, c1).drop(\"genotype\").collect()\n",
    "pd_fronts = get_transformed_front(fronts_a, [\"task\", \"file\"], c0a, c1)\n",
    "reference = all_data.filter(pl.col(\"file\") == \"reference\").drop(\"genotype\").with_columns(\n",
    "    pl.col(c0a).alias(\"c0\"), pl.col(c1).alias(\"c1\")\n",
    ")\n",
    "reference_poly = compute_reference_domination_poly(compute_pareto(reference, [\"task\"], c0a, c1, dmul=-1), [\"task\"], c0a, c1).to_pandas()\n",
    "reference = reference.to_pandas()\n",
    "c0 = c0a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_multiplier = pl.from_pandas(pd_fronts).lazy().select(pl.col(\"c1\").min().log10().floor()).collect()\n",
    "scale_multiplier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i pd_fronts -i reference -i reference_poly -i c0 -i c1 -w 500 -h 300\n",
    "pal <- palette(\"r4\")[2:6]\n",
    "# scientific_10 <- function(x) {   parse(text=gsub(\"e\\\\+*\", \" %*% 10^\", scales::scientific_format()(x))) }\n",
    "scientific_10 <- function(x) { x / 10^9 } \n",
    "\n",
    "# Remove additional samples added to continue the lines to the axis edge\n",
    "pd_fronts_no_ref <- pd_fronts |> filter(`set` != \"reference\")\n",
    "pd_fronts_excl_edges <- pd_fronts_no_ref |> filter(`is_original` > 0.5)\n",
    "\n",
    "pla <- ggplot(pd_fronts_no_ref, aes(x = `c0`, y=`c1`, color=`set`, group=`file`)) +\n",
    "    geom_polygon(data = reference_poly, aes(x=`c0`, y=`c1`), fill=\"orange\", alpha=0.2, inherit.aes=FALSE) +\n",
    "    geom_step(alpha=0.3, direction = \"vh\") +\n",
    "    geom_point(data = pd_fronts_excl_edges, alpha=0.3) +\n",
    "    geom_point(data = reference, color=\"orange\", shape=4, size=2, stroke=2) +\n",
    "    scale_color_manual(values=pal) +\n",
    "    scale_y_continuous(label=scientific_10) +\n",
    "    labs(x = c0, y = bquote(.(c1) ~~ (phantom(\"\")%*% 10^9)), color = \"approach\") +\n",
    "    facet_wrap(`task` ~ ., scales=\"free\") +\n",
    "    theme_bw() +\n",
    "    theme(\n",
    "      legend.position=\"bottom\",\n",
    "      axis.text.x = element_text(angle = 45, vjust = 1, hjust=1),\n",
    "      plot.background = element_rect(fill='transparent', color=NA),\n",
    "      strip.background = element_blank())\n",
    "pla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = datapoints\n",
    "fronts_a = compute_pareto(all_data.lazy(), [\"file\"], c0b, c1).drop(\"genotype\").collect()\n",
    "pd_fronts = get_transformed_front(fronts_a, [\"file\"], c0b, c1)\n",
    "reference = all_data.filter(pl.col(\"file\") == \"reference\").drop(\"genotype\").with_columns(\n",
    "    pl.col(c0b).alias(\"c0\"), pl.col(c1).alias(\"c1\")\n",
    ")\n",
    "reference_poly = compute_reference_domination_poly(compute_pareto(reference, [\"task\"], c0b, c1, dmul=-1), [\"task\"], c0b, c1).to_pandas()\n",
    "reference = reference.to_pandas()\n",
    "c0 = c0b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i pd_fronts -i reference -i reference_poly -i c0 -i c1 -w 500 -h 300\n",
    "\n",
    "# Remove additional samples added to continue the lines to the axis edge\n",
    "pd_fronts_no_ref <- pd_fronts |> filter(`set` != \"reference\")\n",
    "pd_fronts_excl_edges <- pd_fronts_no_ref |> filter(`is_original` > 0.5)\n",
    "\n",
    "plb <- ggplot(pd_fronts_no_ref, aes(x = `c0`, y=`c1`, color=`set`, group=`file`)) +\n",
    "    geom_polygon(data = reference_poly, aes(x=`c0`, y=`c1`), fill=\"orange\", alpha=0.2, inherit.aes=FALSE) +\n",
    "    geom_step(alpha=0.3, direction = \"vh\") +\n",
    "    geom_point(data = pd_fronts_excl_edges, alpha=0.3) +\n",
    "    geom_point(data = reference, color=\"orange\", shape=4, size=2, stroke=2) +\n",
    "    scale_color_manual(values=pal) +\n",
    "    scale_y_continuous(label=scientific_10) +\n",
    "    labs(x = c0, y = bquote(.(c1) ~~ (phantom(\"\")%*% 10^9)), color = \"approach\") +\n",
    "    facet_wrap(`task` ~ ., scales=\"free\") +\n",
    "    theme_bw() +\n",
    "    theme(\n",
    "      legend.position=\"bottom\",\n",
    "      axis.text.x = element_text(angle = 45, vjust = 1, hjust=1),\n",
    "      plot.background = element_rect(fill='transparent', color=NA),\n",
    "      strip.background = element_blank())\n",
    "plb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -w 1000 -h 260\n",
    "pl <- (pla | plb) + plot_layout(guides = \"collect\") & theme(legend.position=\"bottom\")\n",
    "ggsave(\"fronts-in-ab-seg-dominates-parent-marker-rs.pdf\")\n",
    "pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypervolume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frac_offset = 0.01\n",
    "\n",
    "def compute_reference_point_and_norm(df, c0, c1):\n",
    "    return (df.lazy()\n",
    "    # Obtain min / max per task\n",
    "    .group_by(\"task\")\n",
    "    .agg([\n",
    "        ((pl.col(c0) * improvement_direction[c0]).min() * improvement_direction[c0]).alias(\"c0-worst\"),\n",
    "        ((pl.col(c0) * improvement_direction[c0]).max() * improvement_direction[c0]).alias(\"c0-best\"),\n",
    "        ((pl.col(c1) * improvement_direction[c1]).min() * improvement_direction[c1]).alias(\"c1-worst\"),\n",
    "        ((pl.col(c1) * improvement_direction[c1]).max() * improvement_direction[c1]).alias(\"c1-best\"),\n",
    "    ])\n",
    "    # Obtain ranges\n",
    "    .with_columns([\n",
    "        (pl.col(\"c0-best\") - pl.col(\"c0-worst\")).abs().alias(\"c0-range\"),\n",
    "        (pl.col(\"c1-best\") - pl.col(\"c1-worst\")).abs().alias(\"c1-range\"),\n",
    "    ])\n",
    "    # Determine reference point and normalization factor\n",
    "    .select(\n",
    "        pl.col(\"task\"),\n",
    "        (pl.col(\"c0-worst\") - (pl.col(\"c0-range\") * (frac_offset / 2))).alias(\"c0-ref\"),\n",
    "        (pl.col(\"c1-worst\") - (pl.col(\"c1-range\") * (frac_offset / 2))).alias(\"c1-ref\"),\n",
    "        (pl.col(\"c0-range\") * (1.0 + frac_offset)).alias(\"c0-norm\"),\n",
    "        (pl.col(\"c1-range\") * (1.0 + frac_offset)).alias(\"c1-norm\"),\n",
    "    )\n",
    "    )\n",
    "\n",
    "def compute_2d_hv_refdf(df_pareto, ref_df, group_vars, c0, c1):\n",
    "    # note - df_pareto is a df created using compute_pareto\n",
    "    dhva = (df_pareto.lazy()\n",
    "            .join(ref_df, on=\"task\")\n",
    "            .sort(c0, descending=improvement_direction[c0] < 0)\n",
    "        # Samples worse than reference point do not contribute.\n",
    "        # .filter(improvement_direction[c0] * pl.col(c0) > improvement_direction[c0] * ref[0])\n",
    "        # .filter(improvement_direction[c1] * pl.col(c1) > improvement_direction[c1] * ref[1])\n",
    "        .with_columns(\n",
    "        [\n",
    "            maybe_over( improvement_direction[c0] * (pl.col(c0) - pl.col(c0).shift(1).fill_null(pl.col(\"c0-ref\"))) / pl.col(\"c0-norm\"), group_vars).alias(\"slice_width\"),\n",
    "            maybe_over( improvement_direction[c1] * (pl.col(c1) - pl.col(\"c1-ref\")) / pl.col(\"c1-norm\"), group_vars).alias(\"slice_height\"),\n",
    "        ])\n",
    "        .select([pl.col(group_vars), (pl.col(\"slice_width\") * pl.col(\"slice_height\")).alias(\"hv_contrib\")])\n",
    "        .group_by(group_vars).agg(pl.col(\"hv_contrib\").sum())\n",
    "        )\n",
    "    return dhva"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = compute_reference_point_and_norm(datapoints, c0a, c1)\n",
    "hypervolumes = compute_2d_hv_refdf(datapoints, metrics, [\"task\", \"set\", \"file\"], c0a, c1)\n",
    "\n",
    "metrics, hypervolumes = pl.collect_all((metrics, hypervolumes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "with pl.Config(tbl_cols=40, tbl_rows=42) as cfg:\n",
    "    display(hypervolumes.sort([\"task\", \"hv_contrib\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_threshold = 0.05\n",
    "perform_correction = True\n",
    "\n",
    "c_hv_mean = \"mean\"\n",
    "c_hv_std = \"std\"\n",
    "c_hv_q10 = \"q10\"\n",
    "c_hv_median = \"median\"\n",
    "c_hv_q90 = \"q90\"\n",
    "\n",
    "\n",
    "column_ordering = {c_hv_mean: 0, c_hv_std: 1, c_hv_q10: 2, c_hv_median: 3, c_hv_q90: 4 }\n",
    "def sort_key(a):\n",
    "    return [column_ordering.get(r, r) for r in a]\n",
    "\n",
    "hvl = (hypervolumes.lazy()\n",
    "    .rename({\"set\": \"approach\"})\n",
    "    .group_by([\"task\", \"approach\"])\n",
    "    .agg([\n",
    "        pl.col(\"hv_contrib\"),\n",
    "        # pl.col(\"hv_contrib\").mean().alias(c_hv_mean),\n",
    "        # pl.col(\"hv_contrib\").std().alias(c_hv_std),\n",
    "        pl.col(\"hv_contrib\").quantile(0.1).alias(c_hv_q10),\n",
    "        pl.col(\"hv_contrib\").median().alias(c_hv_median),\n",
    "        pl.col(\"hv_contrib\").quantile(0.9).alias(c_hv_q90),\n",
    "    ]))\n",
    "\n",
    "# \n",
    "best = (hvl\n",
    "        .sort(pl.col(\"task\", c_hv_median), descending=True)\n",
    "        .group_by(\"task\")\n",
    "        .first())\n",
    "best.collect()\n",
    "\n",
    "paired = (hvl\n",
    "          .join(best, on=\"task\")\n",
    "          .with_columns(\n",
    "              pl.struct([\"hv_contrib\", \"hv_contrib_right\"]).map_elements(\n",
    "              lambda c: mannwhitneyu(c[\"hv_contrib\"], c[\"hv_contrib_right\"], alternative=\"less\")._asdict()\n",
    "              ,return_dtype=pl.Struct({\"statistic\": pl.Float64, \"pvalue\": pl.Float64})).alias(\"mwu\"))\n",
    "          .select(pl.all().exclude(\"^.*_right$\").exclude(\"mwu\"),\n",
    "                  pl.col(\"mwu\").struct.field(\"statistic\").alias(\"statistic\"),\n",
    "                  pl.col(\"mwu\").struct.field(\"pvalue\").alias(\"pvalue\"),\n",
    "                  (pl.col(\"approach\") == pl.col(\"approach_right\")).alias(\"is_best\"),\n",
    "                  # Don't test against self & reference - testing against self is useless & testing against reference is moot:\n",
    "                  # reference is a distribution that contains only one value - which is extremely annoying to reason about...\n",
    "                  (~((pl.col(\"approach\") == pl.col(\"approach_right\")) | (pl.col(\"approach\") == \"reference\"))).alias(\"include_in_p_test\"),\n",
    "                )\n",
    "          .sort([\"task\", \"pvalue\"])\n",
    "          .with_columns(\n",
    "              (pl.col(\"include_in_p_test\").sum().over(\"task\") - pl.col(\"include_in_p_test\").cum_sum().over(\"task\") + 1).alias(\"c\")\n",
    "          )\n",
    "          .with_columns(\n",
    "              (pl.lit(p_threshold) / (1.0 if not perform_correction else pl.col(\"c\"))).alias(\"p_threshold\")\n",
    "            )\n",
    "          .with_columns(\n",
    "              ((pl.col(\"pvalue\") < pl.col(\"p_threshold\"))).cast(int).cum_min().cast(bool).over(\"task\").alias(\"null reject\")\n",
    "          )\n",
    "          .with_columns(\n",
    "              (~pl.col(\"null reject\") & (pl.col(\"include_in_p_test\") | pl.col(\"is_best\"))).alias(\"near_best\")\n",
    "                  )\n",
    ")\n",
    "# .select(pl.col([\"approach\", \"task\", c_hv_q10, c_hv_median, c_hv_q90, \"near_best\"]))\n",
    "hvl = paired.collect()\n",
    "# incl mean and std: [c_hv_mean, c_hv_std, c_hv_q10, c_hv_median, c_hv_q90]\n",
    "\n",
    "def bold_entry(val):\n",
    "    # Query\n",
    "    near_best = (pl.DataFrame([{\"task\": val.name[0], \"approach\": list(val.index)}])\n",
    "         .lazy()\n",
    "         .explode(\"approach\")\n",
    "         .join(hvl.lazy(), on=[\"task\", \"approach\"], how=\"left\")\n",
    "         .select(pl.col(\"near_best\").fill_null(False))\n",
    "        ).collect()\n",
    "    return [\"font-weight: bold\" if is_near_best else \"\" for is_near_best in near_best[\"near_best\"]]\n",
    "\n",
    "\n",
    "# Pandas has a few more tools for creating tables...\n",
    "hv_table = (hvl\n",
    "    .to_pandas()\n",
    "    .pivot(index=[\"approach\"], columns=[\"task\"], values=[c_hv_q10, c_hv_median, c_hv_q90])\n",
    "    .reorder_levels([1, 0], axis=1)\n",
    "    .sort_index(axis=1, key=sort_key)\n",
    "    .style\n",
    "    .format(na_rep=\"-\", precision=4)\n",
    "    .apply(bold_entry)\n",
    ")\n",
    "hv_table.to_excel(\"./2024-01-15-hypervolume-table-validation-sig.xlsx\")\n",
    "hv_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hvl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(hvl.lazy()\n",
    "    .select(pl.all().exclude([\"hv_contrib\", \"null reject\", \"p_threshold\", \"c\"]))\n",
    "    .sort([\"task\", \"approach\"])\n",
    ").collect().to_pandas().to_excel(\"2024-01-15-hypervolume-table-validation-sig-detailed-supplement.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = compute_reference_point_and_norm(datapoints, c0b, c1)\n",
    "hypervolumes = compute_2d_hv_refdf(compute_pareto(datapoints, [\"task\", \"set\", \"file\"], c0b, c1), metrics, [\"task\", \"set\", \"file\"], c0b, c1)\n",
    "\n",
    "metrics, hypervolumes = pl.collect_all((metrics, hypervolumes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_tab = metrics.to_pandas().style\n",
    "metrics_tab.to_excel(\"2024-01-15-hypervolume-table-test-reference.xlsx\")\n",
    "metrics_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "with pl.Config(tbl_cols=40, tbl_rows=42) as cfg:\n",
    "    display(hypervolumes.sort([\"task\", \"hv_contrib\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_threshold = 0.05\n",
    "perform_correction = True\n",
    "\n",
    "c_hv_mean = \"mean\"\n",
    "c_hv_std = \"std\"\n",
    "c_hv_q10 = \"q10\"\n",
    "c_hv_median = \"median\"\n",
    "c_hv_q90 = \"q90\"\n",
    "\n",
    "\n",
    "column_ordering = {c_hv_mean: 0, c_hv_std: 1, c_hv_q10: 2, c_hv_median: 3, c_hv_q90: 4 }\n",
    "def sort_key(a):\n",
    "    return [column_ordering.get(r, r) for r in a]\n",
    "\n",
    "hvl = (hypervolumes.lazy()\n",
    "    .rename({\"set\": \"approach\"})\n",
    "    .group_by([\"task\", \"approach\"])\n",
    "    .agg([\n",
    "        pl.col(\"hv_contrib\"),\n",
    "        # pl.col(\"hv_contrib\").mean().alias(c_hv_mean),\n",
    "        # pl.col(\"hv_contrib\").std().alias(c_hv_std),\n",
    "        pl.col(\"hv_contrib\").quantile(0.1).alias(c_hv_q10),\n",
    "        pl.col(\"hv_contrib\").median().alias(c_hv_median),\n",
    "        pl.col(\"hv_contrib\").quantile(0.9).alias(c_hv_q90),\n",
    "    ]))\n",
    "\n",
    "# \n",
    "best = (hvl\n",
    "        .sort(pl.col(\"task\", c_hv_median), descending=True)\n",
    "        .group_by(\"task\")\n",
    "        .first())\n",
    "best.collect()\n",
    "\n",
    "paired = (hvl\n",
    "          .join(best, on=\"task\")\n",
    "          .with_columns(\n",
    "              pl.struct([\"hv_contrib\", \"hv_contrib_right\"]).map_elements(\n",
    "              lambda c: mannwhitneyu(c[\"hv_contrib\"], c[\"hv_contrib_right\"], alternative=\"less\")._asdict()\n",
    "              ,return_dtype=pl.Struct({\"statistic\": pl.Float64, \"pvalue\": pl.Float64})).alias(\"mwu\"))\n",
    "          .select(pl.all().exclude(\"^.*_right$\").exclude(\"mwu\"),\n",
    "                  pl.col(\"mwu\").struct.field(\"statistic\").alias(\"statistic\"),\n",
    "                  pl.col(\"mwu\").struct.field(\"pvalue\").alias(\"pvalue\"),\n",
    "                  (pl.col(\"approach\") == pl.col(\"approach_right\")).alias(\"is_best\"),\n",
    "                  # Don't test against self & reference - testing against self is useless & testing against reference is moot:\n",
    "                  # reference is a distribution that contains only one value - which is extremely annoying to reason about...\n",
    "                  (~((pl.col(\"approach\") == pl.col(\"approach_right\")) | (pl.col(\"approach\") == \"reference\"))).alias(\"include_in_p_test\"),\n",
    "                )\n",
    "          .sort([\"task\", \"pvalue\"])\n",
    "          .with_columns(\n",
    "              (pl.col(\"include_in_p_test\").sum().over(\"task\") - pl.col(\"include_in_p_test\").cum_sum().over(\"task\") + 1).alias(\"c\")\n",
    "          )\n",
    "          .with_columns(\n",
    "              (pl.lit(p_threshold) / (1.0 if not perform_correction else pl.col(\"c\"))).alias(\"p_threshold\")\n",
    "            )\n",
    "          .with_columns(\n",
    "              ((pl.col(\"pvalue\") < pl.col(\"p_threshold\"))).cast(int).cum_min().cast(bool).over(\"task\").alias(\"null reject\")\n",
    "          )\n",
    "          .with_columns(\n",
    "              (~pl.col(\"null reject\") & (pl.col(\"include_in_p_test\") | pl.col(\"is_best\"))).alias(\"near_best\")\n",
    "                  )\n",
    ")\n",
    "# .select(pl.col([\"approach\", \"task\", c_hv_q10, c_hv_median, c_hv_q90, \"near_best\"]))\n",
    "hvl = paired.collect()\n",
    "# incl mean and std: [c_hv_mean, c_hv_std, c_hv_q10, c_hv_median, c_hv_q90]\n",
    "\n",
    "def bold_entry(val):\n",
    "    # Query\n",
    "    near_best = (pl.DataFrame([{\"task\": val.name[0], \"approach\": list(val.index)}])\n",
    "         .lazy()\n",
    "         .explode(\"approach\")\n",
    "         .join(hvl.lazy(), on=[\"task\", \"approach\"], how=\"left\")\n",
    "         .select(pl.col(\"near_best\").fill_null(False))\n",
    "        ).collect()\n",
    "    return [\"font-weight: bold\" if is_near_best else \"\" for is_near_best in near_best[\"near_best\"]]\n",
    "\n",
    "\n",
    "# Pandas has a few more tools for creating tables...\n",
    "hv_table = (hvl\n",
    "    .to_pandas()\n",
    "    .pivot(index=[\"approach\"], columns=[\"task\"], values=[c_hv_q10, c_hv_median, c_hv_q90])\n",
    "    .reorder_levels([1, 0], axis=1)\n",
    "    .sort_index(axis=1, key=sort_key)\n",
    "    .style\n",
    "    .format(na_rep=\"-\", precision=4)\n",
    "    .apply(bold_entry)\n",
    ")\n",
    "hv_table.to_excel(\"./2024-01-15-hypervolume-table-test-sig.xlsx\")\n",
    "hv_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hvl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(hvl.lazy()\n",
    "    .select(pl.all().exclude([\"hv_contrib\", \"null reject\", \"p_threshold\", \"c\"]))\n",
    "    .sort([\"task\", \"approach\"])\n",
    ").collect().to_pandas().to_excel(\"2024-01-15-hypervolume-table-test-sig-detailed-supplement.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recombnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
