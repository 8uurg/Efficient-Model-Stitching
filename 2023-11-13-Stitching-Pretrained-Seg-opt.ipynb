{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stitching Segmentation Models\n",
    "\n",
    "Quoting from \"Rethinking Atrous Convolution for Semantic Image Segmentation\":\n",
    "> We evaluate the proposed models on the PASCAL VOC 2012 semantic segmentation benchmark [ 20] which contains 20 foreground object classes and one background class. The original dataset contains 1, 464 (train), 1, 449 (val), and\n",
    "1, 456 (test) pixel-level labeled images for training, validation, and testing, respectively. The dataset is augmented by\n",
    "the extra annotations provided by [ 29 ], resulting in 10, 582 (trainaug) training images. The performance is measured in\n",
    "terms of pixel intersection-over-union (IOU) averaged across the 21 classes.\n",
    "\n",
    "So the VOC dataset may be used.\n",
    "\n",
    "Furthermore, the pretrained models seem to be trained on COCO-using-VOC-labels, we might want to figure that out, too.\n",
    "\n",
    "Alternative: MONAI for Medical Decathlon?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import recomb.problems as problems\n",
    "problem = problems.VOCSegmentationProblem(root=\"<add-dataset-folder>\")\n",
    "# problem.ensure_downloaded()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.datasets as thd\n",
    "import torchvision.models.segmentation as segmentation_models\n",
    "import recomb.layers as ly\n",
    "import igraph as ig\n",
    "import matplotlib.pyplot as plt\n",
    "import polars as pl\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import recomb.cx as cx\n",
    "import recomb.layers as ly\n",
    "import recomb.problems as problems\n",
    "from recomb.cx import forward_get_all_feature_maps, construct_trained_cx_network_stitching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trsf = segmentation_models.DeepLabV3_MobileNet_V3_Large_Weights.COCO_WITH_VOC_LABELS_V1.transforms()\n",
    "from torch.utils.data import DataLoader\n",
    "# d_train, _, _ = problem.load_problem_dataset(transform_train=trsf, transform_validation=trsf)\n",
    "d_train = problem.get_dataset_train()\n",
    "# d_train.transform = trsf\n",
    "dl_train = DataLoader(d_train)\n",
    "it_train = iter(dl_train)\n",
    "for _ in range(5):\n",
    "    X, Y = next(it_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_shape = X.shape\n",
    "in_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_shape = Y.shape\n",
    "out_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stitched_simpl = torch.load(\"stitched-seg.th\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stitchnet, stitchinfo = stitched_simpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "with open(\"g.dot\", \"w\") as f:\n",
    "    stitchnet.to_dot(f, include_ord_label=True)\n",
    "! dot g.dot -Tsvg -og.png\n",
    "SVG(\"g.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cx_connectivity_graph(cx_net: ly.NeuralNetGraph):\n",
    "    g = cx_net.graph.copy()\n",
    "    o = g.topological_sorting()\n",
    "    idxs_to_remove = []\n",
    "    edges_to_add = []\n",
    "    for i in o:\n",
    "        vi = g.vs[i]\n",
    "\n",
    "        is_edge_case = False\n",
    "        if len(vi.in_edges()) == 0:\n",
    "            # edge case - input node\n",
    "            vi[\"cxs\"] = set([vi.index])\n",
    "            is_edge_case = True\n",
    "        \n",
    "        if len(vi.out_edges())  == 0 and vi[\"module\"] < 0:\n",
    "            # edge case - output node\n",
    "            vi[\"cxs\"] = set([vi.index])\n",
    "            is_edge_case = True\n",
    "        \n",
    "        is_cxn = isinstance(cx_net.submodules[vi[\"module\"]], cx.CXN)\n",
    "        cxs_in = set()\n",
    "        # i.e., where does this CXN link to via what node?\n",
    "        affinity_mappings = {}\n",
    "        for e in vi.in_edges():\n",
    "            cxs_in.update(g.vs[e.source][\"cxs\"])\n",
    "            if is_cxn:\n",
    "                affinity_set = affinity_mappings.get(e[\"socket\"], set())\n",
    "                affinity_set.update(g.vs[e.source][\"cxs\"])\n",
    "                affinity_mappings[e[\"socket\"]] = affinity_set\n",
    "        if is_cxn:\n",
    "            edges_to_add += [(s, i, socket) for (socket, cxn_set) in affinity_mappings.items() for s in cxn_set]\n",
    "            vi[\"cxs\"] = set([i])\n",
    "        elif not is_edge_case:\n",
    "            vi[\"cxs\"] = cxs_in\n",
    "            idxs_to_remove.append(i)\n",
    "\n",
    "    g.add_edges([t[:2] for t in edges_to_add], attributes=dict(socket=[t[2] for t in edges_to_add]))\n",
    "    g.delete_vertices(idxs_to_remove)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cxg = get_cx_connectivity_graph(stitchnet)\n",
    "fig, ax = plt.subplots()\n",
    "graph_layout = cxg.layout(\"sugiyama\")\n",
    "graph_layout.rotate(-90)\n",
    "ig.plot(cxg, target=ax, layout=graph_layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_primary_line_membership(cx_graph, verbose=False):\n",
    "    \"\"\"\n",
    "    Recover for each vertex in the crossover point graph to which networks they originally belonged\n",
    "    assuming that input 0 to each crossover point maintains the original graph.\n",
    "\n",
    "    (Note that this method may be skipped by tracking the original origin during stitching and assigning\n",
    "     membership accordingly.)\n",
    "    \"\"\"\n",
    "    # Initialize graph membership to each on their own\n",
    "    cx_graph.vs[\"og\"] = range(len(cx_graph.vs))\n",
    "    # For input & output add a placeholder\n",
    "    cx_graph.vs[0][\"og\"] = -1\n",
    "    cx_graph.vs[1][\"og\"] = -1\n",
    "    \n",
    "    # Go over the graph in a topological order\n",
    "    ordering = cx_graph.topological_sorting()\n",
    "    for o in ordering:\n",
    "        v = cx_graph.vs[o]\n",
    "        # skip over input & output nodes\n",
    "        if v[\"og\"] == -1: continue\n",
    "        # Loop over the elements with a similar affinity set and get their corresponding assignment.\n",
    "        new_og = v[\"og\"]\n",
    "        same_origin_nodes = [e.source for e in v.in_edges() if e[\"socket\"] == 0]\n",
    "        if verbose: print(f\"same origin: {same_origin_nodes}\")\n",
    "        # Merge identities in union-find like structure.\n",
    "        for set_elem in same_origin_nodes:\n",
    "            # print(f\"visiting {set_elem}\")\n",
    "            v_other = cx_graph.vs[set_elem]\n",
    "            if verbose: print(f\"incident edge og is {v_other['og']}\")\n",
    "            if v_other[\"og\"] == -1: continue\n",
    "            new_og = min(new_og, v_other[\"og\"])\n",
    "        if verbose: print(f\"og was {v['og']} should update to {new_og}\")\n",
    "        v[\"og\"] = new_og\n",
    "        for set_elem in same_origin_nodes:\n",
    "            # print(f\"updating {set_elem}\")\n",
    "            v_other = cx_graph.vs[set_elem]\n",
    "            if v_other[\"og\"] == -1: continue\n",
    "            v_id = cx_graph.vs[v_other[\"og\"]]\n",
    "            v_id[\"og\"] = new_og\n",
    "        if verbose: print(f\"og is now {v['og']} updated to {new_og}\")\n",
    "    # iterate union find for each element in the graph.\n",
    "    for o in ordering:\n",
    "        v = cx_graph.vs[o]\n",
    "        # if special case or identical, skip\n",
    "        if v[\"og\"] == -1: continue\n",
    "        if v[\"og\"] == o: continue\n",
    "        # otherwise track down the first identical element\n",
    "        og = v[\"og\"]\n",
    "        while True:\n",
    "            v_potential_og = cx_graph.vs[og]\n",
    "            # found it?\n",
    "            if v_potential_og[\"og\"] == -1: continue\n",
    "            if v_potential_og[\"og\"] == og: break\n",
    "            # otherwise continue down the line\n",
    "            og = v_potential_og[\"og\"]\n",
    "        # go down the line again, updating the og value accordingly.\n",
    "        l = v[\"og\"]\n",
    "        v[\"og\"] = og\n",
    "        while True:\n",
    "            v_other = cx_graph.vs[l]\n",
    "            if v_other[\"og\"] == -1: continue\n",
    "            l = v_other[\"og\"]\n",
    "            v_other[\"og\"] = og\n",
    "            if v_other[\"og\"] == l: break\n",
    "    return cx_graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_primary_line_membership(cxg);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cxg.vs[\"og\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_parallel_set(cxg, i):\n",
    "    s = set(range(len(cxg.vs))) \n",
    "    s -= set(cxg.subcomponent(i, mode='out'))\n",
    "    s -= set(cxg.subcomponent(i, mode='in'))\n",
    "    # s.add(i)\n",
    "    return s\n",
    "\n",
    "def compute_all_parallel_set(cxg):\n",
    "    return [compute_parallel_set(cxg, i) for i in range(len(cxg.vs))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_cxs = compute_all_parallel_set(cxg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "from functools import partial\n",
    "def enumerate_parallel_set(g, set_idx, parallel_sets, i, verbose=False):\n",
    "    def call_funcs(lfn):\n",
    "        for fn in lfn:\n",
    "            fn()\n",
    "\n",
    "    for (set_list, restore_list) in enumerate_parallel_set_recur(g, set_idx, parallel_sets, i, None, set(), verbose=verbose):\n",
    "        yield (lambda: call_funcs(set_list)), (lambda: call_funcs(restore_list))\n",
    "\n",
    "def enumerate_parallel_set_recur(g, set_idx, parallel_sets, i, current_set=None, unpickable: set=set(), ref_og=None, verbose=False):\n",
    "    if current_set is None:\n",
    "        # Initial case - current set is the parallel set of the index we are starting with.\n",
    "        current_set = copy(parallel_sets[i])\n",
    "        ref_og = g.vs[i][\"og\"]\n",
    "        # filter current set based on a match\n",
    "        current_set = {a for a in current_set if g.vs[a][\"og\"] == ref_og}\n",
    "    else:\n",
    "        # Otherwise, update the set of uncovered elements.\n",
    "        current_set = current_set.intersection(parallel_sets[i])\n",
    "\n",
    "    if len(current_set) == 0:\n",
    "        if verbose: print(f\"base case - no other choices necessary after picking {i}\")\n",
    "        # yield setter for configuring and unconfiguring i. No other configurations necessary\n",
    "        # as there are no other branches.\n",
    "        yield [partial(set_idx, i, 1)], [partial(set_idx, i, 0)]\n",
    "        # return - as there are no more elements in the neighborhood.\n",
    "        return\n",
    "\n",
    "    # Obtain a fixed ordering of the set of leftover elements to be picked.\n",
    "    ordering = list(current_set - unpickable)\n",
    "    \n",
    "    # Find current reverse cumulative intersection.\n",
    "    # The intersection of sets picked so far provides knowledge of elements that may need\n",
    "    # to be picked to cover all branches.\n",
    "    # If we perform this operation cumulatively from the right the elements left in the\n",
    "    # set allow us to identify necessary picks.\n",
    "    # if we have the set with fixed ordering [ 1, 2, 3]\n",
    "    # and the set corresponding here are 1 -> {0, 2, 3}, 2 -> {0, 1, 3}, 3 -> {0, 1, 2}\n",
    "    # (note: the index itself is never contained within its own parallel set)\n",
    "    # In this case the sequence of sets would be\n",
    "    # [{}, {1}, {1, 2}]\n",
    "    # as the only set that does not contain {1} is the set corresponding to {1}, 1 must be picked.\n",
    "    cumulative_sets_rl = [None for _ in range(len(ordering))]\n",
    "    cumulative_sets_rl[-1] = current_set.intersection(parallel_sets[ordering[-1]])\n",
    "    required_right = {ordering[-1]}\n",
    "    for i in range(len(ordering) - 1, 0, -1):\n",
    "        el = ordering[i - 1]\n",
    "        cumulative_sets_rl[i - 1] = cumulative_sets_rl[i].intersection(parallel_sets[el])\n",
    "        # note - if an ordering[i - 1] is in cumulative_sets_rl[i], el needs to be picked if we do not\n",
    "        # pick any of the preceding elements as there are no further elements to cover this branch.\n",
    "        if ordering[i - 1] in cumulative_sets_rl[i]:\n",
    "            required_right.add(el)\n",
    "    # If we do it the other direction we can do the same thing for any following elements.\n",
    "    cumulative_sets_lr = [None for _ in range(len(ordering))]\n",
    "    cumulative_sets_lr[0] = current_set.intersection(parallel_sets[ordering[0]])\n",
    "    required_left = {ordering[0]}\n",
    "    for i in range(0, len(ordering) - 1):\n",
    "        el = ordering[i + 1]\n",
    "        cumulative_sets_lr[i + 1] = cumulative_sets_lr[i].intersection(parallel_sets[el])\n",
    "        # similar reasoning - if we pick none of the elements after this one, there would be\n",
    "        # a uncovered branch\n",
    "        if ordering[i + 1] in cumulative_sets_lr[i]:\n",
    "            required_left.add(el)\n",
    "    # Elements that are in both required sets are always to be taken.\n",
    "    always_required = required_left.intersection(required_right)\n",
    "\n",
    "    # For future additions: - if one skips elements that have already been investigated previously (i.e., \n",
    "    # elsewhere in the ordering, another indicator is important to keep track of:\n",
    "    # cumulative_sets_lr[-1] and cumulative_sets_rl[0] should always be empty sets - if they are not\n",
    "    # there exists an element that is not optional that was excluded.\n",
    "    # Probably shouldn't happen since we force always required, but just in case, handle this edge case.\n",
    "    if len(cumulative_sets_lr[-1]) != 0 or len(cumulative_sets_rl[0]) != 0:\n",
    "        if verbose: print(\"forbidden case - no choices cover all branches anymore...\")\n",
    "        return\n",
    "\n",
    "    fixed_set = [partial(set_idx, i, 1)]\n",
    "    fixed_restore = [partial(set_idx, i, 0)]\n",
    "    \n",
    "    if verbose: print(f\"in this case to cover all branches {always_required} are required\")\n",
    "    for a in always_required:\n",
    "        current_set.intersection_update(parallel_sets[a])\n",
    "        fixed_set.append(partial(set_idx, a, 1))\n",
    "        fixed_restore.append(partial(set_idx, a, 0))\n",
    "\n",
    "    if len(current_set) == 0:\n",
    "        if verbose: print(f\"fixed case - no more free choices left to make after picking {i}\")\n",
    "        yield fixed_set, fixed_restore\n",
    "    else:\n",
    "        if verbose: print(f\"recursive case for {i}\")\n",
    "        for e in current_set:\n",
    "            # consider the cases where we pick it\n",
    "            \n",
    "            print(f\"considering picking {e}\")\n",
    "            for (set_list, restore_list) in enumerate_parallel_set_recur(g, parallel_sets, e, current_set, unpickable=unpickable, ref_og=ref_og):\n",
    "                yield (fixed_set + set_list), (fixed_restore + restore_list)\n",
    "            print(f\"no longer considering picking {e}\")\n",
    "            # now - for the following picks consider the case where we not allow e to be picked anymore.\n",
    "            unpickable.add(i)\n",
    "        # to avoid issues with branching allow picking these elements again if another branch investigates them.\n",
    "        for e in current_set:\n",
    "            unpickable.remove(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_idx(i, to):\n",
    "    module_idx = cxg.vs[i]['module']\n",
    "    switch = stitchnet.submodules[module_idx]\n",
    "    switch.active = to\n",
    "    print(f\"submodule {module_idx} ({type(switch)}) active set to {to}\")\n",
    "\n",
    "\n",
    "for set_list, restore_list in enumerate_parallel_set(cxg, set_idx, parallel_cxs, 16):\n",
    "    set_list()\n",
    "    print(\"----------------------\")\n",
    "    restore_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "num_orig_nodes = len(cxg.vs)\n",
    "graph_layout, cxge = cxg.layout_sugiyama(return_extended_graph=True)\n",
    "# Transform graph layout to coords\n",
    "layout_coords = np.array(graph_layout)\n",
    "vertex_coords = np.array([layout_coords[v.index] for v in cxg.vs])\n",
    "# edge_coords = [[layout_coords[e.source, :], layout_coords[e.target, :], [np.nan, np.nan]] for e in cxg.es]\n",
    "edge_coords = [[\n",
    "    layout_coords[e.source, :],\n",
    "    layout_coords[e.target, :],\n",
    "    [np.nan, np.nan],\n",
    "    ]\n",
    "    for e in cxge.es]\n",
    "edge_coords = np.array(edge_coords).reshape(-1, 2)\n",
    "is_arrow_end = np.array([[0, 0, 0] if edge.target >= num_orig_nodes else [0, 10, 0] for edge in cxge.es]).ravel()\n",
    "\n",
    "# edge_trace = go.Scatter(x=edge_x, y=edge_y, line=dict(width=lineWidth, color=lineColor), hoverinfo='none', mode='lines')\n",
    "edge_trace = go.Scatter(\n",
    "    # note: coords are transposed.\n",
    "    x=edge_coords[:, 1],\n",
    "    y=edge_coords[:, 0],\n",
    "    hoverinfo='none',\n",
    "    mode='lines+markers',\n",
    "    marker=dict(\n",
    "        size=10,\n",
    "        angleref=\"previous\",\n",
    "        symbol=\"arrow\",\n",
    "        ),\n",
    "    marker_size = is_arrow_end,\n",
    ")\n",
    "\n",
    "def color_table(ogv):\n",
    "    if ogv == 2: return 'red'\n",
    "    if ogv == 3: return 'blue'\n",
    "    return 'white'\n",
    "\n",
    "# node_trace = go.Scatter(x=node_x, y=node_y, mode='markers', hoverinfo='text', marker=dict(showscale=False, color = nodeColor, size=nodeSize))\n",
    "node_trace = go.Scatter(\n",
    "    # note: coords are transposed.\n",
    "    x=vertex_coords[:, 1],\n",
    "    y=vertex_coords[:, 0],\n",
    "    marker_color = [color_table(og) for og in cxg.vs[\"og\"]],\n",
    "    mode='markers',\n",
    "    # hoverinfo='text',\n",
    "    marker=dict(showscale=False)\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=[edge_trace, node_trace],\n",
    "             layout=go.Layout(\n",
    "                showlegend=False,\n",
    "                # hovermode='closest',\n",
    "                margin=dict(b=20,l=5,r=5,t=40),\n",
    "                xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "                yaxis=dict(showgrid=False, zeroline=False, showticklabels=False))\n",
    "                )\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed computational cost info\n",
    "import torchinfo\n",
    "import recomb.eval_costs as ec\n",
    "cost_summary = torchinfo.summary(stitchnet, input_data=[X])\n",
    "ec.embed_cost_stats_in_model(cost_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((\"The stitched neural network has \"\n",
    "       f\"{len(stitchinfo.joiners)} matches\"\n",
    "       ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate predetermined neighborhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stitcher_genotype_cardinality(stitchnet, stitcher):\n",
    "    # Note: has been copied to cx.py.\n",
    "\n",
    "    for v in stitchnet.graph.vs:\n",
    "        if v[\"module\"] < 0: continue\n",
    "        md = stitchnet.submodules[v[\"module\"]]\n",
    "        if not isinstance(md, cx.CXN): continue\n",
    "        sockets = np.unique([e[\"socket\"] for e in v.in_edges()])\n",
    "        md.input_sockets = sockets\n",
    "        md.num_input_sockets = len(sockets)\n",
    "\n",
    "    o = []\n",
    "    for js in stitcher.joiners:\n",
    "        for j in js:\n",
    "            o.append(j.num_input_sockets)\n",
    "    o.append(stitcher.output_switch.num_input_sockets)\n",
    "    return o\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ealib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The scheduler is the object that manages evaluations, and how they are \n",
    "# performed in parallel. Note, generally quite simple by deferring to other\n",
    "# scheduler implementations (e.g. python's asyncio loop & ray)\n",
    "# In this case we do defer to pythons async loop. \n",
    "eq = ealib.PythonAsyncIOEQ()\n",
    "scheduler = ealib.Scheduler(eq)\n",
    "population_size = 5\n",
    "num_clusters = 3\n",
    "objective_indices = [0, 1]\n",
    "initializer = ealib.CategoricalUniformInitializer()\n",
    "fos_learner = ealib.CategoricalLinkageTree(\n",
    "    ealib.NMI(),\n",
    "    ealib.FoSOrdering.Random,\n",
    "    prune_root=True,\n",
    ")\n",
    "acceptance_criterion = ealib.ScalarizationAcceptanceCriterion(\n",
    "    ealib.TschebysheffObjectiveScalarizer(objective_indices)\n",
    ")\n",
    "archive = ealib.BruteforceArchive(objective_indices)\n",
    "\n",
    "gomea = ealib.DistributedAsynchronousGOMEA(scheduler, population_size, num_clusters, objective_indices, initializer, fos_learner, acceptance_criterion, archive )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def noop(genotype):\n",
    "    return 0.0\n",
    "alphabet_size = get_stitcher_genotype_cardinality(stitchnet, stitchinfo)\n",
    "l = len(alphabet_size)\n",
    "problem_template = ealib.DiscreteObjectiveFunction(noop, l, alphabet_size, 0)\n",
    "\n",
    "async def evaluate_objective(p: ealib.Population, i: ealib.Individual):\n",
    "    global count\n",
    "    genotype = p.getData(ealib.GENOTYPECATEGORICAL, i)\n",
    "    genotype_array = np.asarray(genotype)\n",
    "\n",
    "    # :)\n",
    "    accuracy = 0.95\n",
    "    cost = 100_051_124\n",
    "    \n",
    "    # Store result\n",
    "    objective = p.getData(ealib.OBJECTIVE, i)\n",
    "    objective.set_objective(0, accuracy)\n",
    "    objective.set_objective(1, cost)\n",
    "\n",
    "objective_fn = ealib.PyAsyncObjectiveFunction(scheduler, problem_template, evaluate_objective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "stepper = ealib.TerminationStepper((lambda : gomea), 100)\n",
    "f = ealib.SimpleConfigurator(objective_fn, stepper, seed)\n",
    "f.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "stitchnet.cpu()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this network will cost  70200072306 mul-adds and approx 3032042904\n",
    "# this network will cost 180146119266 mul-adds and approx 3761352936 of memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "with open(\"g.dot\", \"w\") as f:\n",
    "    stitchnet.to_dot(f, include_ord_label=True)\n",
    "! dot g.dot -Tsvg -og.png\n",
    "SVG(\"g.png\")\n",
    "\n",
    "\n",
    "total_mult_adds, total_bytes = ec.evaluate_compute_cost(stitchnet)\n",
    "print(f\"this network will cost {total_mult_adds} mul-adds and approx {total_bytes} of memory\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stitchinfo.output_switch.active = 2\n",
    "stitchinfo.output_switch.simplify = True\n",
    "\n",
    "for j in stitchinfo.joiners:\n",
    "        j[0].active = 0\n",
    "        j[1].active = 0\n",
    "        j[0].simplify = True\n",
    "        j[1].simplify = True\n",
    "\n",
    "dev = torch.device(\"cuda:1\")\n",
    "\n",
    "stitchnet_pruned = stitchnet.to_graph()\n",
    "stitchnet_pruned.prune_unused()\n",
    "\n",
    "total_mult_adds, total_bytes = ec.evaluate_compute_cost(stitchnet_pruned)\n",
    "print(f\"this network will cost {total_mult_adds} mul-adds and approx {total_bytes} of memory\")\n",
    "\n",
    "\n",
    "from IPython.display import SVG\n",
    "with open(\"g.dot\", \"w\") as f:\n",
    "    stitchnet_pruned.to_dot(f, include_ord_label=True)\n",
    "! dot g.dot -Tsvg -og.png\n",
    "SVG(\"g.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = problems.VOCSegmentationProblem(\"<add-dataset-folder>\", batched_validation=True, validation_sample_limit=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = torch.device(\"cuda:1\")\n",
    "# Evaluate reference networks\n",
    "reference_q = []\n",
    "stitchinfo.output_switch.active = 2\n",
    "stitchinfo.output_switch.simplify = True\n",
    "\n",
    "for j in stitchinfo.joiners:\n",
    "        j[0].active = 0\n",
    "        j[1].active = 0\n",
    "        j[0].simplify = True\n",
    "        j[1].simplify = True\n",
    "\n",
    "batch_size = 16\n",
    "stitchnet_pruned = stitchnet.to_graph()\n",
    "stitchnet_pruned.prune_unused()\n",
    "total_mult_adds, total_bytes = ec.evaluate_compute_cost(stitchnet_pruned)\n",
    "print(f\"full ensemble will cost {total_mult_adds} mul-adds and approx {total_bytes} of memory\")\n",
    "neti_os = NeuralNetIndividual(stitchnet_pruned)\n",
    "accuracy, loss = problem.evaluate_network(dev, neti_os, batch_size=batch_size, objective=\"both\")\n",
    "reference_q.append((accuracy, loss, total_bytes, total_mult_adds, cx.convert_stitcher_to_genotype(stitchinfo, stringify=False)))\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "stitchinfo.output_switch.active = 1\n",
    "stitchnet_pruned = stitchnet.to_graph()\n",
    "stitchnet_pruned.prune_unused()\n",
    "total_mult_adds, total_bytes = ec.evaluate_compute_cost(stitchnet_pruned)\n",
    "\n",
    "neti_os = NeuralNetIndividual(stitchnet_pruned)\n",
    "accuracy, loss = problem.evaluate_network(dev, neti_os, batch_size=batch_size, objective=\"both\")\n",
    "reference_q.append((accuracy, loss, total_bytes, total_mult_adds, cx.convert_stitcher_to_genotype(stitchinfo, stringify=False)))\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "stitchinfo.output_switch.active = 0\n",
    "stitchnet_pruned = stitchnet.to_graph()\n",
    "stitchnet_pruned.prune_unused()\n",
    "total_mult_adds, total_bytes = ec.evaluate_compute_cost(stitchnet_pruned)\n",
    "\n",
    "neti_os = NeuralNetIndividual(stitchnet_pruned)\n",
    "accuracy, loss = problem.evaluate_network(dev, neti_os, batch_size=batch_size, objective=\"both\")\n",
    "reference_q.append((accuracy, loss, total_bytes, total_mult_adds, cx.convert_stitcher_to_genotype(stitchinfo, stringify=False)))\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensembles_a = []\n",
    "ensembles_b = []\n",
    "start_a_end_b = []\n",
    "start_b_end_a = []\n",
    "\n",
    "# note - usually 1, but due to the large amount of matches, this has been\n",
    "# increased so that we can evaluate blocks of solutions instead.\n",
    "step = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stitchinfo.output_switch.active = 2\n",
    "stitchinfo.output_switch.simplify = True\n",
    "\n",
    "dev = torch.device(\"cuda:0\")\n",
    "\n",
    "for j in stitchinfo.joiners:\n",
    "        j[0].active = 0\n",
    "        j[1].active = 0\n",
    "        j[0].simplify = True\n",
    "        j[1].simplify = True\n",
    "\n",
    "i = 0\n",
    "j = stitchinfo.joiners[i]\n",
    "j[0].active = 0\n",
    "j[1].active = 1\n",
    "\n",
    "stitchnet_pruned = stitchnet.to_graph()\n",
    "stitchnet_pruned.prune_unused()\n",
    "\n",
    "# Get compute & memory requirements\n",
    "# s = torchinfo.summary(stitchnet_pruned, input_data=[X])\n",
    "total_mult_adds, total_bytes = ec.evaluate_compute_cost(stitchnet_pruned)\n",
    "\n",
    "neti_os = NeuralNetIndividual(stitchnet_pruned)\n",
    "\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "j[0].active = 0\n",
    "j[1].active = 0\n",
    "\n",
    "from IPython.display import SVG\n",
    "with open(\"g.dot\", \"w\") as f:\n",
    "    stitchnet_pruned.to_dot(f, include_ord_label=True)\n",
    "! dot g.dot -Tsvg -og.png\n",
    "SVG(\"g.png\")\n",
    "\n",
    "# accuracy, loss = problem.evaluate_network(dev, neti_os, batch_size=batch_size, objective=\"both\")\n",
    "# ensembles_a.append((accuracy, loss, total_bytes, total_mult_adds, cx.convert_stitcher_to_genotype(stitchinfo, stringify=False)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate neighborhood of networks\n",
    "def evaluate_neighborhood_given_offset(offset, batch_size=16):\n",
    "    stitchinfo.output_switch.active = 2\n",
    "    stitchinfo.output_switch.simplify = True\n",
    "\n",
    "    dev = torch.device(\"cuda:0\")\n",
    "\n",
    "    for j in stitchinfo.joiners:\n",
    "            j[0].active = 0\n",
    "            j[1].active = 0\n",
    "            j[0].simplify = True\n",
    "            j[1].simplify = True\n",
    "\n",
    "    for i in range(offset, len(stitchinfo.joiners), step):\n",
    "        j = stitchinfo.joiners[i]\n",
    "        j[0].active = 0\n",
    "        j[1].active = 1\n",
    "\n",
    "        stitchnet_pruned = stitchnet.to_graph()\n",
    "        stitchnet_pruned.prune_unused()\n",
    "\n",
    "        # Get compute & memory requirements\n",
    "        # s = torchinfo.summary(stitchnet_pruned, input_data=[X])\n",
    "        total_mult_adds, total_bytes = ec.evaluate_compute_cost(stitchnet_pruned)\n",
    "\n",
    "        neti_os = NeuralNetIndividual(stitchnet_pruned)\n",
    "        accuracy, loss = problem.evaluate_network(dev, neti_os, batch_size=batch_size, objective=\"both\")\n",
    "        ensembles_a.append((accuracy, loss, total_bytes, total_mult_adds, cx.convert_stitcher_to_genotype(stitchinfo, stringify=False)))\n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        j[0].active = 0\n",
    "        j[1].active = 0\n",
    "\n",
    "    for i in range(offset, len(stitchinfo.joiners), step):\n",
    "        j = stitchinfo.joiners[i]\n",
    "        j[0].active = 1\n",
    "        j[1].active = 0\n",
    "\n",
    "        stitchnet_pruned = stitchnet.to_graph()\n",
    "        stitchnet_pruned.prune_unused()\n",
    "\n",
    "        # Get compute & memory requirements\n",
    "        # s = torchinfo.summary(stitchnet_pruned, input_data=[X])\n",
    "        total_mult_adds, total_bytes = ec.evaluate_compute_cost(stitchnet_pruned)\n",
    "\n",
    "        neti_os = NeuralNetIndividual(stitchnet_pruned)\n",
    "        accuracy, loss = problem.evaluate_network(dev, neti_os, batch_size=batch_size, objective=\"both\")\n",
    "        ensembles_b.append((accuracy, loss, total_bytes, total_mult_adds, cx.convert_stitcher_to_genotype(stitchinfo, stringify=False)))\n",
    "        \n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        j[0].active = 0\n",
    "        j[1].active = 0\n",
    "\n",
    "    stitchinfo.output_switch.active = 1\n",
    "    for i in range(offset, len(stitchinfo.joiners), step):\n",
    "        j = stitchinfo.joiners[i]\n",
    "        j[0].active = 0\n",
    "        j[1].active = 1\n",
    "\n",
    "        stitchnet_pruned = stitchnet.to_graph()\n",
    "        stitchnet_pruned.prune_unused()\n",
    "\n",
    "        # Get compute & memory requirements\n",
    "        # s = torchinfo.summary(stitchnet_pruned, input_data=[X])\n",
    "        total_mult_adds, total_bytes = ec.evaluate_compute_cost(stitchnet_pruned)\n",
    "\n",
    "        neti_os = NeuralNetIndividual(stitchnet_pruned)\n",
    "        accuracy, loss = problem.evaluate_network(dev, neti_os, batch_size=batch_size, objective=\"both\")\n",
    "        start_a_end_b.append((accuracy, loss, total_bytes, total_mult_adds, cx.convert_stitcher_to_genotype(stitchinfo, stringify=False)))\n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        j[0].active = 0\n",
    "        j[1].active = 0\n",
    "\n",
    "    stitchinfo.output_switch.active = 0\n",
    "    for i in range(offset, len(stitchinfo.joiners), step):\n",
    "        j = stitchinfo.joiners[i]\n",
    "        j[0].active = 1\n",
    "        j[1].active = 0\n",
    "\n",
    "        stitchnet_pruned = stitchnet.to_graph()\n",
    "        stitchnet_pruned.prune_unused()\n",
    "\n",
    "        # Get compute & memory requirements\n",
    "        # s = torchinfo.summary(stitchnet_pruned, input_data=[X])\n",
    "        total_mult_adds, total_bytes = ec.evaluate_compute_cost(stitchnet_pruned)\n",
    "\n",
    "        neti_os = NeuralNetIndividual(stitchnet_pruned)\n",
    "        accuracy, loss = problem.evaluate_network(dev, neti_os, batch_size=batch_size, objective=\"both\")\n",
    "        start_b_end_a.append((accuracy, loss, total_bytes, total_mult_adds, cx.convert_stitcher_to_genotype(stitchinfo, stringify=False)))\n",
    "        \n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        j[0].active = 0\n",
    "        j[1].active = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note - very little training performed above initially\n",
    "evaluate_neighborhood_given_offset(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_neighborhood_given_offset(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_neighborhood_given_offset(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_neighborhood_given_offset(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_neighborhood_given_offset(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_neighborhood_given_offset(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_neighborhood_given_offset(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_neighborhood_given_offset(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_neighborhood_given_offset(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_neighborhood_given_offset(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_schema = [\"accuracy\", \"loss\", \"total bytes\", \"multiply-adds\", \"genotype\"]\n",
    "samples_reference = pl.DataFrame(reference_q, schema=df_schema).\\\n",
    "    with_columns([\n",
    "        pl.lit(pl.Series([\"ensemble\", \"b\", \"a\"])).alias(\"set\"),\n",
    "        pl.lit(False).alias(\"contains stitch\"),\n",
    "    ])\n",
    "samples_ensemble_a = pl.DataFrame(ensembles_a, schema=df_schema).\\\n",
    "    with_columns([\n",
    "        pl.lit(\"ensemble-major-a\").alias(\"set\"),\n",
    "        pl.lit(True).alias(\"contains stitch\"),\n",
    "    ])\n",
    "samples_ensemble_b = pl.DataFrame(ensembles_b, schema=df_schema).\\\n",
    "    with_columns([\n",
    "        pl.lit(\"ensemble-major-b\").alias(\"set\"),\n",
    "        pl.lit(True).alias(\"contains stitch\"),\n",
    "    ])\n",
    "samples_ab = pl.DataFrame(start_a_end_b, schema=df_schema).\\\n",
    "    with_columns([\n",
    "        pl.lit(\"stitch-a-to-b\").alias(\"set\"),\n",
    "        pl.lit(True).alias(\"contains stitch\"),\n",
    "    ])\n",
    "samples_ba = pl.DataFrame(start_b_end_a, schema=df_schema).\\\n",
    "    with_columns([\n",
    "        pl.lit(\"stitch-b-to-a\").alias(\"set\"),\n",
    "        pl.lit(True).alias(\"contains stitch\"),\n",
    "    ])\n",
    "\n",
    "samples = pl.concat([\n",
    "    samples_reference,\n",
    "    samples_ensemble_a,\n",
    "    samples_ensemble_b,\n",
    "    samples_ab,\n",
    "    samples_ba,\n",
    "], how=\"vertical_relaxed\").with_columns(\n",
    "    pl.col(\"loss\").clip(0.0, 4.0).alias(\"loss-clip\")\n",
    ")\n",
    "samples.write_ipc(\"segmentation-stitch-samples.arrow\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot approximation front"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = pl.read_ipc(\"resnet-efficientnet-stitch-samples.arrow\")\n",
    "\n",
    "# Extract some rows of reference interest\n",
    "dfcna = samples[2]\n",
    "dfcnb = samples[1]\n",
    "dfcnens = samples[0]\n",
    "\n",
    "# \n",
    "improvement_direction = {\n",
    "    \"accuracy\": 1,\n",
    "    \"loss\": -1,\n",
    "    \"loss-clip\": -1,\n",
    "    \"total bytes\": -1,\n",
    "    \"multiply-adds\": -1,\n",
    "    # \"genotype\": 0, # -- not a criterion\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(stitchinfo.joiners) * 4 * 15 / 60 / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # How many seconds per evaluated sample?\n",
    "# number_of_minutes = 16 * 8\n",
    "# number_of_seconds = 0\n",
    "# number_of_samples = len(samples) - 3\n",
    "# seconds_total = number_of_minutes * 60 + number_of_seconds\n",
    "# seconds_per_sample = seconds_total / number_of_samples\n",
    "\n",
    "# print(f\"spent {number_of_minutes}m{number_of_seconds}s \"\n",
    "#       f\"to evaluate {number_of_samples} samples.\\n\"\n",
    "#       f\"Resulting in a cost of {seconds_per_sample}s per sample.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute pareto set from these points, with respect to these\n",
    "# two criteria / objectives\n",
    "c0 = \"accuracy\"\n",
    "c1 = \"multiply-adds\"\n",
    "\n",
    "samples_pareto = (samples.lazy()\n",
    "    .sort(c0, descending=improvement_direction[c0] > 0)\n",
    "    .with_columns((pl.col(c1) * -improvement_direction[c1]).alias(\"c1-min\"))\n",
    "    .with_columns((pl.col(\"c1-min\")).cummin().alias(\"mv\"))\n",
    "    .with_columns((pl.col(\"c1-min\") < pl.col(\"mv\").shift(1)).alias(\"is pareto\")).fill_null(True)\n",
    "    .filter(pl.col(\"is pareto\"))\n",
    ").collect()\n",
    "\n",
    "samples_pareto_stitch_only = (samples.lazy()\n",
    "    .filter(pl.col(\"contains stitch\"))\n",
    "    .sort(c0, descending=improvement_direction[c0] > 0)\n",
    "    .with_columns((pl.col(c1) * -improvement_direction[c1]).alias(\"c1-min\"))\n",
    "    .with_columns((pl.col(\"c1-min\")).cummin().alias(\"mv\"))\n",
    "    .with_columns((pl.col(\"c1-min\") < pl.col(\"mv\").shift(1)).alias(\"is pareto\")).fill_null(True)\n",
    "    .filter(pl.col(\"is pareto\"))\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sn, df in samples.filter(pl.col(\"contains stitch\")).group_by(\"set\", maintain_order=True):\n",
    "    plt.scatter(df[c0], df[c1], label=sn, s=1.0)\n",
    "\n",
    "plt.scatter(samples_pareto[c0], samples_pareto[c1], alpha=0.4, marker=\"s\", color=\"grey\")\n",
    "plt.scatter(samples_pareto_stitch_only[c0], samples_pareto_stitch_only[c1], s=20.0, alpha=0.5, color=\"grey\")\n",
    "\n",
    "plt.scatter(dfcna[c0], dfcna[c1], label=\"a\", marker='x')\n",
    "plt.scatter(dfcnb[c0], dfcnb[c1], label=\"b\", marker='x')\n",
    "plt.scatter(dfcnens[c0], dfcnens[c1], label=\"ensemble\", marker='x')\n",
    "\n",
    "def get_direction_arrow(c):\n",
    "    return '->' if improvement_direction[c] > 0 else '<-'\n",
    "\n",
    "plt.xlabel(f\"{c0} ({get_direction_arrow(c0)})\")\n",
    "plt.ylabel(f\"{c1} ({get_direction_arrow(c1)})\")\n",
    "plt.legend(loc='upper left',\n",
    "           bbox_to_anchor=(1.0, 1.0),\n",
    "           fancybox=False,\n",
    "           shadow=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Potential Points of Improvement?**\n",
    "1. Pretrain for longer? (e.g. specific stopping condition?)\n",
    "2. Train using actual loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist2d(samples[c0], samples[c1])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some test evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev2 = torch.device(\"cuda:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate original networks\n",
    "neti_a = NeuralNetIndividual(gca)\n",
    "neti_b = NeuralNetIndividual(gcb)\n",
    "problem.evaluate_network(dev2, neti_a, objective=\"both\"),\\\n",
    "    problem.evaluate_network(dev, neti_b, objective=\"both\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stitchneti = NeuralNetIndividual(stitchnet)\n",
    "for j in stitchinfo.joiners:\n",
    "    j[0].active = 0\n",
    "    j[1].active = 0\n",
    "\n",
    "stitchinfo.output_switch.active = 0\n",
    "roa = problem.evaluate_network(dev, stitchneti, objective=\"both\")\n",
    "stitchinfo.output_switch.active = 1\n",
    "rob = problem.evaluate_network(dev, stitchneti, objective=\"both\")\n",
    "roa, rob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stitchneti = NeuralNetIndividual(stitchnet)\n",
    "for j in stitchinfo.joiners:\n",
    "    j[0].active = 0\n",
    "    j[1].active = 0\n",
    "stitchinfo.output_switch.active = 2\n",
    "j = stitchinfo.joiners[18]\n",
    "# j[0].active = 0\n",
    "# j[1].active = 1\n",
    "j[0].active = 1\n",
    "j[1].active = 0\n",
    "\n",
    "problem.evaluate_network(dev, stitchneti, objective=\"both\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recombnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
